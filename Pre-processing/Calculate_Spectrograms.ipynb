{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Calculate_Spectrograms.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOorpMNO/M0refu1yJVqb8w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"g1SpxETtZnEl"},"source":["# Calculate log mel-Spectrograms\n","\n","Logarithmic mel-spectrograms are calculated according to Palanisamy et al. (2020) for three channels with different window sizes and hop lengths:\n","* Channel 1: window size = 25ms, hop length = 10ms.\n","* Channel 2: window size = 50ms, hop length = 25ms.\n","* Channel 3: window size = 100ms, hop length = 50ms. "]},{"cell_type":"markdown","metadata":{"id":"QESDw2xzJNSe"},"source":["# Libraries"]},{"cell_type":"code","metadata":{"id":"bAylpMZQN5WP"},"source":["!pip install torch torchvision torchaudio\n","import torch\n","print (\"current pytorch version is: \", torch.__version__)\n","\n","import PIL\n","print (\"current pillow version is: \", PIL.__version__)\n","\n","from google.colab import drive \n","import librosa\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import torchaudio\n","from torchvision import transforms, models, datasets\n","from PIL import Image\n","import pickle as pkl # save files als pkl\n","from tqdm import tqdm # Progress Bar\n","import warnings # ignore librosa warnings\n","print('\\ndone importing.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1u2ouWLS3MX_"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"vo_kLmfPlr9z"},"source":["def extract_spectrogram(audio_path,audiofile_name,target,resize,sampling_rate):\n","  \"\"\"\n","  # audio_path: path to audio files\n","  # audiofile_name: name of audio file (e.g. 'test.mp3')\n","  # target: emotion class of audio\n","  # sampling_rate: sampling rate of audio file\n","\n","  # Reference: Palanisamy et al. (2020).\n","  \"\"\"\n","\n","  num_channels = 3\n","  window_sizes = [25, 50, 100]\n","  hop_sizes = [10, 25, 50]\n","\n","  # Load audio file\n","  with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\")\n","        try:\n","          clip, sr = librosa.load(\"{}/{}\".format(audio_path, audiofile_name), sr=sampling_rate)\n","        except Exception as e:\n","          print(e)\n","\n","  # Initialise spectrogram list\n","  specs = []\n","\n","  for i in range(num_channels):\n","    window_length = int(round(window_sizes[i]*sampling_rate/1000))\n","    hop_length = int(round(hop_sizes[i]*sampling_rate/1000))\n","\n","    clip = torch.Tensor(clip)\n","\n","    # Calculate Log Mel-Spectrograms\n","    try:\n","      spec = torchaudio.transforms.MelSpectrogram(sample_rate=sampling_rate, n_fft=4410, win_length=window_length, hop_length=hop_length, n_mels=128)(clip)\n","    except Exception as e:\n","      print(e)\n","    \n","    eps = 1e-6\n","    spec = spec.numpy()\n","    spec = np.log(spec+ eps)\n","    spec = np.asarray(transforms.Resize(resize)(Image.fromarray(spec)))\n","    specs.append(spec)\n","\n","  # Create new entry for spectrogram value list\n","  new_entry = {}\n","  new_entry[\"audio\"] = audiofile_name\n","  new_entry[\"target\"] = target # Emotion label\n","  new_entry[\"spec_values\"] = np.array(specs) # Values for 3-channel Log Mel-Spectrograms\n","  return new_entry"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_e4-fxlEocsQ"},"source":["def calculate_specs(df,audio_path,sampling_rate,resize):\n","\n","  print()\n","  print('-'*20)\n","  print('Input size: {}'.format(resize))\n","\n","  # Initialise list for spectrogram values\n","  values = []\n","\n","  with tqdm(total=df.shape[0]) as t: # Prograss Bar\n","    for row in df.itertuples():\n","      \"\"\"\n","      ATTENTION: \n","      Index for filename in row might differ \n","      depending on the dataset used!\n","      \"\"\"\"\n","      audiofile_name = str(row[1])\n","\n","      # Emotion label numerical \n","      # (0: sad, 1: delighted, 2: relaxed, 3: angry)\n","      emotion_num = row[6]\n","\n","      try:\n","        new_entry = extract_spectrogram(audio_path,audiofile_name,emotion_num,resize,sampling_rate)\n","      except Exception as e:\n","        print(e)\n","      \n","      values.append(new_entry)\n","      t.update() # Prograss Bar Update\n","  t.close()\n","\n","  out_pkl_file = '/path/to/log_melspectrograms.pkl')\n","\n","  # Save spectrograms as pkl file\n","  with open(out_pkl_file,\"wb\") as handler:\n","    try:\n","      pkl.dump(values, handler, protocol=pkl.HIGHEST_PROTOCOL)\n","    except Exception as e:\n","      print(e)\n","\n","    print('\\nFile saved to {}'.format(out_pkl_file))\n","    print()\n","\n","  return values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABoelPYsL95P"},"source":["# Script\n","\n","Execute to calculate Log Mel-spectrograms."]},{"cell_type":"code","metadata":{"id":"SCnhBStVlLkv"},"source":["# Get Data from Google Drive\n","drive.mount('/content/gdrive')\n","\n","df = pd.read_csv('/path/to/list_of_audiofiles.csv',index_col=0)\n","\n","audiofiles_path = '/path/to/audiofiles'\n","\n","resize = (224,224) # ImageNet input image size\n","sampling_rate = 44100 \n","\n","values = []\n","\n","try:\n","  val = calculate_specs(df,audiofiles_path,sampling_rate,resize)\n","except Exception as e:\n","  print(e)\n","  val = 0\n","\n","values.append(val)"],"execution_count":null,"outputs":[]}]}