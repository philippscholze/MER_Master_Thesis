{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbTXNS1EjUMy"
   },
   "source": [
    "# Create combined Dataset\n",
    "\n",
    "With this notebook, a dataset combining log mel-spectrograms and tokenized lyrics is created. Attention: you need to calcuclte the log mel-spectrograms first and store them in a pkl-file (see `CalculateSpectrograms.ipynb`-file)! They are needed to run this script correctly :) \n",
    "\n",
    "The resulting dataset can be used for AudioNet, LyricsNet and FusionNet models for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzPDsshKj3WN"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cR6CIv2DoTZA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # handling csv data\n",
    "import pickle as pkl # handle pkl data\n",
    "import numpy as np # numerical operations\n",
    "from tqdm import tqdm # progress bar\n",
    "import time # measure processing time\n",
    "from google.colab import drive # connect to Google Drive\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "!pip install transformers\n",
    "!pip install sentencepiece # needed for XLNet\n",
    "# XLNet \n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, XLNetModel, XLNetConfig\n",
    "# Padding/Truncating sequence to MAX_LEN\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1rU9Vi0kCmR"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-MjQ4Udo8Ga"
   },
   "outputs": [],
   "source": [
    "def load_pkl_data(pkl_dir):\n",
    "  t_start = time.time()\n",
    "  print('\\nLoading data...')\n",
    "  with open(pkl_dir, \"rb\") as f:\n",
    "    try:\n",
    "      data = pkl.load(f)\n",
    "      print('\\nData loaded successfully from \\n{}!'.format(pkl_dir))\n",
    "      print('\\nNumber of data entries: {}'.format(len(data)))\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "  t_end = time.time()\n",
    "  t_elapsed = t_end - t_start\n",
    "  print('\\nTime elapsed: {} seconds.'.format(np.round(t_elapsed,2)))\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "def load_csv_data(csv_dir):\n",
    "  data = pd.read_csv(csv_dir,index_col=0)\n",
    "\n",
    "  t_start = time.time()\n",
    "  print('\\nLoading data...')\n",
    "  try:\n",
    "    data = pd.read_csv(csv_dir,index_col=0)\n",
    "    print('\\nData loaded successfully from \\n{}!'.format(csv_dir))\n",
    "    print('\\nNumber of data entries: {}'.format(data.shape[0]))\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "  t_end = time.time()\n",
    "  t_elapsed = t_end - t_start\n",
    "  print('\\nTime elapsed: {} seconds.'.format(np.round(t_elapsed,2)))\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkIF3Y_XkF9g"
   },
   "source": [
    "# Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnm-QrsC1jQX"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "def get_tokenizer(transformer_name):\n",
    "  if transformer_name == \"XLNet\":\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "  elif transformer_name == \"BERT\":\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "  else:\n",
    "    raise ValueError(\"transformer_name must be set to 'XLNet' or 'BERT'!\")\n",
    "  return tokenizer\n",
    "\n",
    "\n",
    "def preprocess_lyrics(lyrics,tokenizer,transformer_name,MAX_LEN):\n",
    "  # Tokenize lyrics\n",
    "  if transformer_name == \"XLNet\":\n",
    "    tokenized_text = [tokenizer.tokenize(x) for x in lyrics]\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]\n",
    "  elif transformer_name == \"BERT\":\n",
    "    input_ids = [tokenizer.encode(x,add_special_tokens=True) for x in lyrics]\n",
    "  else:\n",
    "    raise ValueError(\"transformer_name must be set to 'XLNet' or 'BERT'!\")\n",
    "\n",
    "  # Padding/Truncating\n",
    "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "  # Creating attention masks for input_ids\n",
    "  attention_masks = create_attention_masks(input_ids)\n",
    "\n",
    "  return input_ids, attention_masks\n",
    "\n",
    "\n",
    "# Attention Mask for XLNet\n",
    "def create_attention_masks(input_ids):\n",
    "  attention_masks = []\n",
    "  # Create a mask of 1s for each token followed by 0s for padding\n",
    "  for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "  return attention_masks\n",
    "\n",
    "\n",
    "# Normalize each channel of the spectrogram to [0, 1] values\n",
    "def normalize_specs(spec):\n",
    "  spec_norm = []\n",
    "  for channel in spec:\n",
    "    channel_norm = (channel-np.min(channel))/np.max(channel-np.min(channel))\n",
    "    spec_norm.append(channel_norm)\n",
    "\n",
    "  return np.asarray(spec_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI7pxxz0lslB"
   },
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsPkEX6godmJ"
   },
   "outputs": [],
   "source": [
    "# Connect to google drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "dataset_csv = '/path/to/lyrics_lemma_no_sw.csv'\n",
    "dataset_pkl = '/path/to/log_melspectrograms.pkl'\n",
    "\n",
    "# Load lyrics data\n",
    "data_csv = load_csv_data(dataset_csv)\n",
    "# Load log mel-spectrograms data\n",
    "data_pkl = load_pkl_data(dataset_pkl)\n",
    "\n",
    "\n",
    "# Pre-process Lyrics\n",
    "transformer_name = 'XLNet'\n",
    "MAX_LEN = 160\n",
    "\n",
    "# Get lyrics from CSV dataset\n",
    "lyrics = data_csv['lyrics_lemma_no_sw'].tolist()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = get_tokenizer(transformer_name)\n",
    "\n",
    "# Pre-processing lyrics\n",
    "input_ids, attention_masks = preprocess_lyrics(lyrics,tokenizer,transformer_name,MAX_LEN)\n",
    "\n",
    "\n",
    "# Combine data to a single dataset (combined Dataset)\n",
    "idx = 0\n",
    "for row in data_csv.itertuples():\n",
    "  \"\"\"\n",
    "  ATTENTION: \n",
    "  Index for filename in row might differ \n",
    "  depending on the dataset used!\n",
    "  \"\"\"\"\n",
    "\n",
    "  # Load name of audiofile from metadata set\n",
    "  audiofile_name = str(row[1])\n",
    "\n",
    "  # Add audiofile name to combined dataset\n",
    "  data_pkl[idx]['filename'] = audiofile_name\n",
    "  # Add normalized spectrograms\n",
    "  data_pkl[idx]['spec_values'] = normalize_specs(data_pkl[idx]['spec_values'])\n",
    "  # Add input IDS (lyrics)\n",
    "  data_pkl[idx]['input_ids'] = input_ids[idx]\n",
    "  # Add attention masks (lyrics)\n",
    "  data_pkl[idx]['attention_mask'] = attention_masks[idx]\n",
    "  # Add name of tokenizer (lyrics)\n",
    "  data_pkl[idx]['tokenizer'] = 'XLNet'\n",
    "  # Add maximum length of token sequence (lyrics)\n",
    "  data_pkl[idx]['MAX_LEN'] = MAX_LEN\n",
    "\n",
    "  idx += 1\n",
    "\n",
    "out_pkl_file = '/path/to/store/combined_dataset.pkl'\n",
    "\n",
    "# Save dataset (log mel-spectrograms and tokenized lyrics) to pkl-file\n",
    "with open(out_pkl_file,\"wb\") as handler:\n",
    "  try:\n",
    "    pkl.dump(data_pkl, handler, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "  print('\\nFile saved to {}\\n'.format(out_pkl_file))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNFpcQddj5MKiQOFLgKkmRg",
   "collapsed_sections": [],
   "name": "Create_Combined_Dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
