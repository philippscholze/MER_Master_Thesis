{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Lyrics\n",
    "\n",
    "This notebook is pre-processing the lyrics. At first, structural information like [Chorus] etc. and punctuations are removed from the lyrics. Then a spell check is applied and all capital letters are replaced by lowercase letters. After this first pre-processing step, the lyrics are lemmatized using the WordNet Lemmatizer. Finally, all stopwords are removed from the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lyrics(lyrics,spell):\n",
    "    # Flatten lyrics\n",
    "    lyrics_flat = lyrics.replace('\\r', '\\n').replace('\\n', ' ').lower()\n",
    "    lyrics_flat = ' ' + lyrics_flat + ' '\n",
    "    \n",
    "    # Remove special cases (English)\n",
    "    lyrics_flat = lyrics_flat.replace(\"â€™\", \"'\")\n",
    "    lyrics_flat = lyrics_flat.replace(\"'m \", \" am \")\n",
    "    lyrics_flat = lyrics_flat.replace(\"'re \", \" are \")\n",
    "    lyrics_flat = lyrics_flat.replace(\"'ve \", \" have \")\n",
    "    lyrics_flat = lyrics_flat.replace(\"'d \", \" would \")\n",
    "    lyrics_flat = lyrics_flat.replace(\"'ll \", \" will \")\n",
    "    lyrics_flat = lyrics_flat.replace(\" he's \", \" he is \")\n",
    "    lyrics_flat = lyrics_flat.replace(\" she's \", \" she is \")\n",
    "    lyrics_flat = lyrics_flat.replace(\" it's \", \" it is \")\n",
    "    lyrics_flat = lyrics_flat.replace(\" ain't \", \" is not \")\n",
    "    lyrics_flat = lyrics_flat.replace(\"can't \", \" cannot \")\n",
    "    lyrics_flat = lyrics_flat.replace(\"won't \", \" will not \")\n",
    "    lyrics_flat = lyrics_flat.replace(\"'s \", \" \")\n",
    "    lyrics_flat = lyrics_flat.replace(\"n't \", \" not \")\n",
    "    lyrics_flat = lyrics_flat.replace(\" 'cause \", \" because \")\n",
    "    lyrics_flat = lyrics_flat.replace(\" 'til \", \" until \")\n",
    "    \n",
    "    # Remove structural information (like [Chorus] etc.)\n",
    "    lyrics_flat = re.sub(\"([\\[]).*?([\\]])\", \"\\g<1>\\g<2>\", lyrics_flat)\n",
    "    \n",
    "    # Remove punctuation and weird signs except apostrophe (like \"nothin'\" etc.)\n",
    "    punctuation = (',', '\"', \",\", ';', ':', '.', '?', '!', '(', ')',\n",
    "               '{', '}', '/', '\\\\', '_', '|', '-', '@', '#', '*','[',']')\n",
    "    for p in punctuation:\n",
    "        lyrics_flat = lyrics_flat.replace(p, '')\n",
    "        \n",
    "    # Spell checking\n",
    "    lyrics_tokens = lyrics_flat.split()\n",
    "    for i in range(0,len(lyrics_tokens)):\n",
    "        lyrics_tokens[i] = spell.correction(lyrics_tokens[i])\n",
    "    \n",
    "    # Remove all remaining apostrophes\n",
    "    lyrics_flat = ' '.join(lyrics_tokens)\n",
    "    lyrics_flat = lyrics_flat.replace(\"''\", \"\")\n",
    "    lyrics_tokens = lyrics_flat.split()\n",
    "    \n",
    "    # Join tokens to string\n",
    "    lyrics_flat = ' '.join(lyrics_tokens)\n",
    "    \n",
    "    return lyrics_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Lyrics\n",
    "\n",
    "- Remove structural information like [Chorus], punctuation\n",
    "- Spell checking\n",
    "- All lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw lyrics data\n",
    "data = pd.read_csv('/path/to/lyrics_data.csv',index_col=0)\n",
    "\n",
    "# Initialising\n",
    "lyrics_preprocessed = []\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Pre-process lyrics\n",
    "for row in data.itertuples():\n",
    "     \"\"\"\n",
    "    ATTENTION: \n",
    "    Index for filename in row might differ \n",
    "    depending on the dataset used!\n",
    "    \"\"\"\"\n",
    "    lyrics = row[7]\n",
    "    tokens = preprocess_lyrics(lyrics,spell)\n",
    "    lyrics_preprocessed.append(tokens)\n",
    "\n",
    "# Add to dataframe\n",
    "data['lyrics_preprocessed'] = lyrics_preprocessed\n",
    "\n",
    "# Save to csv file\n",
    "data.to_csv(r'./path/to/lyrics_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing and removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lyrics_lemma_no_sw = []\n",
    "\n",
    "for row in data.itertuples():\n",
    "    \"\"\"\n",
    "    ATTENTION: \n",
    "    Index for filename in row might differ \n",
    "    depending on the dataset used!\n",
    "    \"\"\"\"\n",
    "    tokens = row[9].split()\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    tokens_lemma = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
    "    # Remove stopwords\n",
    "    tokens_lemma = [word for word in tokens_lemma if not word in stopwords.words()]\n",
    "    # Join tokens to string\n",
    "    lyrics_lemma = ' '.join(tokens_lemma)\n",
    "    # Add to data\n",
    "    lyrics_lemma_ohne_sw.append(lyrics_lemma)\n",
    "\n",
    "# Add to data\n",
    "data['lyrics_lemma_no_sw'] = lyrics_lemma_no_sw\n",
    "\n",
    "data.to_csv(r'./path/to/lyrics_lemma_no_sw.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
