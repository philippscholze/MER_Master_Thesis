{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AudioNet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyMFMtdXd4XuH8NPrM5FYNNM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U9EhPbYWTv-o"},"source":["# Libraries"]},{"cell_type":"code","metadata":{"id":"7U6C1SIc35FE"},"source":["# General libraries\n","import pandas as pd # Handling csv data\n","import pickle as pkl # Handle pkl data\n","import numpy as np # Numerical operations\n","print('\\nCurrent Numpy version: {}'.format(np.__version__))\n","from tqdm import tqdm # Progress bar\n","import time # Measure processing time\n","from google.colab import drive # Connect to Google Drive\n","import warnings\n","import datetime\n","\n","# Plotting\n","import seaborn as sn\n","import matplotlib\n","import matplotlib.pyplot as plt\n","print('\\nCurrent matplotlib version: {}'.format(matplotlib.__version__))\n","from matplotlib.pyplot import cm # Colors for loss plot\n","%matplotlib inline\n","\n","# Reading and displaying images\n","from skimage.io import imread\n","from skimage.transform import resize\n","\n","import sklearn\n","print('\\nCurrent sklearn version: {}'.format(sklearn.__version__))\n","\n","# Splitting dataset\n","from sklearn.model_selection import train_test_split\n","# Creating k-fold cross validation sets\n","from sklearn.model_selection import KFold\n","\n","# Evaluating the model\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","# PyTorch libraries and modules\n","!pip install torch torchvision torchaudio\n","import torch\n","print (\"\\nCurrent PyTorch version is: \", torch.__version__)\n","from torch.autograd import Variable\n","\n","# Network architecture\n","import torch.nn as nn\n","from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n","# Optimizers\n","from torch.optim import Adam, SGD, RMSprop\n","from torch.utils.data import Dataset, DataLoader\n","# Torchvision for pre-trained models\n","from torchvision import models\n","# For normalizing the spectrograms \n","from torchvision import transforms\n","\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","!pip install transformers\n","!pip install sentencepiece # needed for XLNet\n","import transformers\n","print (\"\\nCurrent transformers version is: \", transformers.__version__)\n","# XLNet \n","from transformers import XLNetTokenizer, XLNetForSequenceClassification, XLNetModel, XLNetConfig\n","# BERT\n","from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, BertModel\n","# Utils\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","# Padding/Truncating sequence to MAX_LEN\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# Tensorflow for GPU access\n","import tensorflow as tf\n","print (\"\\nCurrent TensorFlow version is: \", tf.__version__)\n","\n","print('\\nImporting libraries completed!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqdPGkFXXTeU"},"source":["# Helper Functions"]},{"cell_type":"code","metadata":{"id":"kUN5KkXspFf-"},"source":["# Loading data from pkl-files\n","def load_pkl_data(pkl_dir):\n","  t_start = time.time()\n","  print('\\nLoading data...')\n","\n","  with open(pkl_dir, \"rb\") as f:\n","    try:\n","      data = pkl.load(f)\n","      print('\\nData loaded successfully from \\n{}!'.format(pkl_dir))\n","      print('\\nNumber of data entries: {}'.format(len(data)))\n","    except Exception as e:\n","      print(e)\n","\n","  t_end = time.time()\n","  t_elapsed = t_end - t_start\n","  print('\\nTime elapsed: {} seconds.'.format(np.round(t_elapsed,2)))\n","\n","  return data\n","\n","\n","# Creating unique folder for storing the model and metrics \n","def create_model_folder_name(dataset_name,saved_models_path):\n","  currentDT = datetime.datetime.now()\n","  folder_name = 'AudioNet{}_{}'.format(dataset_name,currentDT.strftime(\"%Y%m%d_%H-%M-%S\"))\n","\n","  if not os.path.isdir(saved_models_path+folder_name):\n","    os.mkdir(saved_models_path+folder_name)\n","  else:\n","    warnings.warn('Folder {} already exists!'.format(saved_models_path+folder_name))\n","  \n","  return folder_name\n","\n","\n","# Defines a custom Log-file to keep track of actions\n","class Logger():\n","  def __init__(self):\n","    self.logs = {}\n","\n","  def add(self,key,value):\n","    self.logs[str(key)] = value\n","\n","  def __getitem__(self, key):\n","    entry = self.logs[key]\n","    return entry\n","\n","  def save_log(self,path):\n","    longest_key = max(len(key) for key in self.logs)\n","    with open('{}Log_File.txt'.format(path), 'w') as f:\n","      f.write('{:=^60}\\n\\n'.format(' LOG FILE '))\n","      for key in self.logs:\n","        f.write('{:>{}}: {}\\n\\n'.format(str(key),longest_key,str(self.logs[key])))\n","      f.write('{:=^60}\\n\\n'.format(' END OF LOG '))\n","\n","\n","# Returns average loss for each training epoch (Palanisamy et al., 2020)\n","class RunningAverage():\n","\tdef __init__(self):\n","\t\tself.total = 0\n","\t\tself.steps = 0\n","\n","\tdef update(self, loss):\n","\t\tself.total += loss\n","\t\tself.steps += 1\n","\n","\tdef __call__(self):\n","\t\treturn (self.total/float(self.steps))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xss0ttMFrdEU"},"source":["# Set the device to the available GPU from Google Colab\n","def get_GPU_device():\n","  # Get the GPU device name.\n","  device_name = tf.test.gpu_device_name()\n","\n","  GPU_name = \"N/A\"\n","\n","  if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","  else:\n","    print('GPU device not found.')\n","    print('Maybe check if GPU is available for this notebook:')\n","    print('Edit ==> Notebook-Preferences ==> Hardware Acceleration ==> choose GPU!')\n","\n","  # Check if GPU is available\n","  if torch.cuda.is_available():    \n","  \n","    device = torch.device(\"cuda\")\n","    print('\\nThere are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU: {}\\n'.format(torch.cuda.get_device_name(0)))\n","    GPU_name = str(torch.cuda.get_device_name(0))\n","\n","  else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","    \n","  return device, GPU_name"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5_aMSgnh59Z"},"source":["# Cross-Validation"]},{"cell_type":"code","metadata":{"id":"92u6cEsBh6Lb"},"source":["# Create k-fold Cross Validation splits\n","def create_kfold_CV(data,num_folds,random_state,shuffle):\n","  kf = KFold(n_splits=num_folds,random_state=random_state,shuffle=shuffle)\n","  fold = 1\n","\n","  for train_index, test_index in kf.split(data):\n","    for idx in test_index:\n","      data[idx][\"fold\"] = fold\n","    fold += 1\n","  \n","  return data\n","\n","\n","\"\"\"\n","Determine which samples are used for training and validation \n","depending on fold number\n","\"\"\"\n","def get_train_val_folds(train_data,fold):\n","  val_fold = []\n","  train_fold = []\n","  \n","  for d in train_data:\n","    if d[\"fold\"] == fold:\n","      val_fold.append(d)\n","    else:\n","      train_fold.append(d)\n","\n","  return train_fold,val_fold"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uP4X14PyT1DD"},"source":["# Define Models"]},{"cell_type":"markdown","metadata":{"id":"-SozdKk-yE4y"},"source":["## AudioNet\n","\n","Creates a DenseNet201 model"]},{"cell_type":"code","metadata":{"id":"wkd013uG4Y7P"},"source":["class AudioNet(nn.Module):\n","  def __init__(self,num_classes,pretrained=True):\n","    super(AudioNet, self).__init__()\n","    # Load model from pretrained\n","    if pretrained == True:\n","      self.model = models.densenet201(pretrained=pretrained)\n","      self.model.classifier = nn.Linear(1920, num_classes)\n","      # Freeze weights in model except classification layer\n","      for name, param in self.model.named_parameters():\n","        if 'classifier' not in name:\n","          param.requires_grad = False\n","        else:\n","          param.requires_grad = True\n","    # Randomized weights (untrained)\n","    else:\n","      self.model = models.densenet201()\n","      self.model.classifier = nn.Linear(1920, num_classes)\n","\n","  def forward(self, x):\n","    output = self.model(x)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vK5DVZfsyHHJ"},"source":["## LyricsNet\n","\n","Creates a XLNet model"]},{"cell_type":"code","metadata":{"id":"wXCUfH4t4zmJ"},"source":["class LyricsNet(nn.Module):\n","  def __init__(self,num_classes,pretrained=True):\n","    super(LyricsNet, self).__init__()\n","    if pretrained == True:\n","      # Load model from pretrained\n","      self.model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\",num_labels=num_classes)\n","      # Freeze weights in model except classification layer\n","      for name, param in self.model.named_parameters():\n","        if 'logits_proj' not in name:\n","          param.requires_grad = False\n","        else:\n","          param.requires_grad = True\n","    else:\n","      # Randomized weights (untrained)\n","      self.config = XLNetConfig()\n","      self.config.num_labels = num_classes\n","      self.model = XLNetForSequenceClassification(config=self.config)\n","\n","  def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n","    output = self.model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels) \n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4Cm08p2yI1x"},"source":["## FusionNet\n","\n","Combines AudioNet and LyricsNet with an extra classifier layer on top"]},{"cell_type":"code","metadata":{"id":"ukS2nKCF424_"},"source":["class FusionNet(nn.Module):\n","  def __init__(self,num_classes,pretrained=True,dropout_p=0.1):\n","    super(FusionNet, self).__init__()\n","    # Define audio branch\n","    self.AudioNet = AudioNet(num_classes=64,pretrained=pretrained)\n","    # Define lyrics branch\n","    self.LyricsNet = LyricsNet(num_classes=64,pretrained=pretrained)\n","    # Define classifier\n","    self.classifier = nn.Sequential(nn.Linear(128, 128),\n","                                    nn.ReLU(),\n","                                    nn.Dropout(dropout_p),\n","                                    nn.Linear(128,num_classes))\n","    \n","  def forward(self, specs, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n","    AudioNet_outputs = self.AudioNet(specs)\n","    LyricsNet_outputs = self.LyricsNet(input_ids, token_type_ids=None, attention_mask=None, labels=None)\n","    output = torch.cat((AudioNet_outputs, LyricsNet_outputs.logits), dim=1)\n","    output = self.classifier(output)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"250tzfNI4XAK"},"source":["# Dataset Preparation"]},{"cell_type":"markdown","metadata":{"id":"eK0zLMnLT5cQ"},"source":["## Define Audio Dataset\n","\n","Expected dataset contains list of dictionaries `d`. Each dictionary should have the following keys:\n","* `d[\"spec_values\"]` 3-channel values of log mel-spectrogram\n","* `d[\"input_ids\"]` tokenized and converted lyrics\n","* `d[\"attention_mask\"]` corresponding attention mask for transformer net\n","* `d[\"target\"]` numeric values of emotion label (0: sad, 1: delighted, 2: relaxed, 3: angry)\n","* `d[\"filename\"]` spotify ID used as filename\n","* `d[\"tokenizer\"]` type of tokenizer used for preprocessing lyrics\n","* `d[\"MAX_LEN\"]` maximum number of tokens per lyrics \n","* `d[\"audio\"]` name of audio file (**deprecated**: use filename instead!)"]},{"cell_type":"code","metadata":{"id":"o7CR1r9ZT7yt"},"source":["class AudioDataset(Dataset):\n","  def __init__(self, data, transforms=None):\n","    self.length = 224\n","    self.transforms = transforms\n","    self.data = data\n","\n","  def __len__(self):\n","    return len(self.data)\n","    \n","  def __getitem__(self, idx):\n","    entry = self.data[idx]\n","\n","    # Audio Log Mel-Spectrograms\n","    spec_values = np.array(entry[\"spec_values\"])\n","    # Reshape from (224, 224, 3) to (3, 224, 224)\n","    spec_values = spec_values.reshape(-1, 224, self.length)\n","    # Convert to tensor\n","    spec_values = torch.Tensor(spec_values)\n","    if self.transforms:\n","      spec_values = self.transforms(spec_values)\n","\n","    # Numeric emotion labels\n","    target = [int(entry[\"target\"])]\n","    target = torch.LongTensor(target)\n","\n","    return (spec_values, target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wbcUllDayRgh"},"source":["## Dataset Helper Functions"]},{"cell_type":"code","metadata":{"id":"Ethue02jZG8b"},"source":["def fetch_audio_dataloader(data,batch_size,mode):\n","\n","  # ImageNet mean and standard deviation values\n","  mean = [0.485, 0.456, 0.406]\n","  std = [0.229, 0.224, 0.225]\n","\n","  # Create dataset\n","  data = AudioDataset(data, transforms=transforms.Normalize(mean=mean,std=std))\n","\n","  # Create sampler\n","  if mode == 'train':\n","    sampler = RandomSampler(data)\n","  elif mode == 'val':\n","    sampler = SequentialSampler(data)\n","  else:\n","    raise ValueError(\"mode must be set to 'train' or 'val'!\")\n","\n","  # Create dataloader\n","  dataloader = DataLoader(data,sampler=sampler,batch_size=batch_size)\n","\n","  return dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqUU_rjiz5TC"},"source":["# Create train and test set\n","def split_data(data,split_size,random_state,num_folds):\n","  X = [] # Specs (Log-Melspectrograms)\n","  Y = [] # Targets (emotion labels)\n","\n","  for d in data:\n","    new_entry = {}\n","    X.append(d[\"spec_values\"]) # Spectrogram\n","    new_entry[\"target\"] = d[\"target\"] # Emotion label\n","    Y.append(new_entry)\n","  \n","  # Split into train and test set\n","  train_X, test_X, train_Y, test_Y = train_test_split(X, \n","                                                      Y,\n","                                                      random_state=random_state,\n","                                                      test_size=split_size)\n","  # Merge values and targets for dataloader\n","  train = merge_data(train_X,train_Y)\n","  test = merge_data(test_X,test_Y)\n","\n","  # Prepare k-fold cross-validation\n","  train = create_kfold_CV(data=train,num_folds=num_folds,random_state=random_state,shuffle=True)\n","\n","  # Output\n","  return train, test\n","\n","\n","# Merge values and targets for dataloader\n","def merge_data(specs,targets):\n","  merged_data = []\n","\n","  for spec in specs:\n","    entry = {}\n","    entry[\"spec_values\"] = spec\n","    merged_data.append(entry)\n","\n","  for i in range(0,len(targets)):\n","    merged_data[i][\"target\"] = targets[i][\"target\"] # Emotion label\n","  \n","  return merged_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GZdRyjUynHN"},"source":["# Metrics"]},{"cell_type":"code","metadata":{"id":"yY_yzDqmrrQ0"},"source":["def create_metrics(config,targets_true, targets_pred, fold, save_metrics=False):\n","  # Emotion labels\n","  target_names = ['sad', 'delighted', 'relaxed','angry']\n","\n","  print('\\n{:=^60}\\n'.format(' CLASSIFICATION REPORT '))\n","  print(classification_report(targets_true, targets_pred, target_names=target_names))\n","\n","  # Confusion matrix\n","  conf_mat=confusion_matrix(targets_true, targets_pred)\n","\n","  print('\\n{:=^60}\\n'.format(' CONFUSION MATRIX '))\n","  print(conf_mat)\n","\n","  # Save Classification Report & Confusion Matrix to separate files\n","  if save_metrics:\n","    save_classification_report(config,targets_true,targets_pred,target_names,fold)\n","    save_confusion_matrix_to_txt(config,conf_mat,target_names,fold)\n","\n","  plot_confusion_matrix(config,conf_mat,target_names,fold,save_metrics=save_metrics)\n","\n","\n","def plot_train_loss(epochs,train_loss_total,config,fold,save_metrics=False):\n","  # Training loss for all folds\n","  plt.figure(figsize = (8,6))\n","  plt.plot(np.arange(1,epochs+1),train_loss_total,linewidth=2.5)\n","  plt.title('Training Loss (Fold {})'.format(fold))\n","  plt.grid(True)\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Loss')\n","  plt.tight_layout()\n","  if save_metrics:\n","    path = config[\"saved_models_path\"]+config[\"model_folder_name\"]+'/'\n","    plt.savefig('{}Training_Loss_Fold{}.pdf'.format(path,fold))\n","\n","\n","def plot_confusion_matrix(config,conf_mat,target_names,fold,save_metrics=False):\n","  df_cm = pd.DataFrame(conf_mat, index = [i for i in target_names],\n","                    columns = [i for i in target_names])\n","  plt.figure(figsize = (8,6))\n","  sn.heatmap(df_cm, annot=True,cmap=\"YlGnBu\",fmt='g')\n","  plt.title('Confusion Matrix (Fold {})'.format(fold))\n","  plt.xlabel('predicted label')\n","  plt.ylabel('true label')\n","  plt.tight_layout()\n","  if save_metrics:\n","    path = config[\"saved_models_path\"]+config[\"model_folder_name\"]+'/'\n","    plt.savefig('{}Confusion_Matrix_Plot_Fold{}.pdf'.format(path,fold))\n","\n","\n","def save_confusion_matrix_to_txt(config,conf_mat,target_names,fold):\n","\n","  path = config[\"saved_models_path\"]+config[\"model_folder_name\"]+'/'\n","\n","  with open('{}Confusion_Matrix_Fold{}.txt'.format(path,fold), 'w') as f:\n","    f.write('{:=^60}\\n\\n'.format(' CONFUSION MATRIX '))\n","    for i in range(0,len(conf_mat)):\n","      f.write('{:>10} {}\\n'.format(str(target_names[i]),str(conf_mat[i])))\n","\n","\n","def save_classification_report(config,targets_true,targets_pred,target_names,fold):\n","  report = classification_report(targets_true, targets_pred, output_dict=True, target_names=target_names)\n","\n","  path = config[\"saved_models_path\"]+config[\"model_folder_name\"]+'/'\n","\n","  df = pd.DataFrame(report).transpose()\n","  df.style.set_caption(\"Classification Report\")\n","  df.to_csv('{}Classification_Report_Fold{}.csv'.format(path,fold))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8CUxnVxpypKD"},"source":["# Train and evaluate AudioNet model (Functions)"]},{"cell_type":"code","metadata":{"id":"15dQaykIq86B"},"source":["def train_AudioNet(model, device, dataloader, optimizer, loss_fn):\n","  # Set model to training mode\n","  model.train()\n","  \n","  loss_avg = RunningAverage()\n","\n","  # Train the model\n","  with tqdm(total=len(dataloader)) as t:\n","    for batch_idx, data in enumerate(dataloader):\n","      spec_values = data[0].to(device)\n","      targets = data[1].squeeze(1).to(device)\n","\n","      model.zero_grad()\n","\n","      outputs = model(spec_values)\n","      \n","      loss = loss_fn(outputs, targets)\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      loss_avg.update(loss.item())\n","\n","      t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n","      t.update()\n","\n","  return loss_avg()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9m3mV0FzZp5"},"source":["def evaluate_AudioNet(model, device, dataloader):\n","  # Set model to evaluation mode\n","  model.eval()\n","\n","  correct = 0\n","  total = 0\n","\n","  targets_pred=torch.zeros(0,dtype=torch.long, device='cpu')\n","  targets_true=torch.zeros(0,dtype=torch.long, device='cpu') \n","\n","  with tqdm(total=len(dataloader)) as t:\n","    with torch.no_grad():\n","      for batch_idx, data in enumerate(dataloader):\n","        spec_values = data[0].to(device)\n","        targets = data[1].squeeze(1).to(device)\n","\n","        outputs = model(spec_values)\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        # Concatenate batch prediction results\n","        targets_pred=torch.cat([targets_pred,predicted.view(-1).cpu()])\n","        targets_true=torch.cat([targets_true,targets.view(-1).cpu()])\n","\n","        total += targets.size(0)\n","        correct += (predicted == targets).sum().item()\n","        t.set_postfix(accuracy='{:05.3f}'.format(100*correct/total))\n","        t.update()\n","    \n","  # Convert to numpy array (for metrics)\n","  targets_true = targets_true.numpy()\n","  targets_pred = targets_pred.numpy()\n","\n","  # Calculate accuracy\n","  accuracy = (100*correct/total)\n","\n","  return accuracy, targets_true, targets_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRI3gmRdr537"},"source":["def create_optimizer(model,learning_rate):\n","  param_optimizer = list(model.named_parameters())\n","  no_decay = ['bias', 'gamma', 'beta']\n","  optimizer_grouped_parameters = [\n","      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","      'weight_decay_rate': 0.01},\n","      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","      'weight_decay_rate': 0.0}\n","  ]\n","  optimizer = AdamW(optimizer_grouped_parameters,lr=learning_rate)\n","  return optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZKNka-pvftC"},"source":["def train_eval_AudioNet(model,device,train_data,val_data,config,logger,fold,save_model=False):\n","\n","  train_dataloader = fetch_audio_dataloader(data=train_data,batch_size=config[\"batch_size\"],mode='train')\n","  val_dataloader = fetch_audio_dataloader(data=val_data,batch_size=config[\"batch_size\"],mode='val')\n","\n","  optimizer = create_optimizer(model,config[\"learning_rate\"])\n","\n","  loss_fn = nn.CrossEntropyLoss()\n","\n","  total_loss = []\n","\n","  epochs = config[\"epochs\"]\n","\n","  if config[\"scheduler\"]:\n","    # Total number of training steps is number of batches * number of epochs.\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler.\n","    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n","\n","  # TRAINING\n","  print('\\n{:=^60}\\n'.format(' TRAINING '))\n","  logger.add('TRAINING Fold {}'.format(fold),'\\n{:=^60}'.format(''))\n","\n","  t_start_train = time.time()\n","\n","  for epoch in range(1,epochs+1):\n","    print('{:-^60}\\n'.format(' Epoch {}/{} '.format(epoch,epochs)))\n","    t_start_epoch = time.time()\n","    avg_loss = train_AudioNet(model=model, device=device, dataloader=train_dataloader, optimizer=optimizer, loss_fn=loss_fn)\n","    t_end_epoch = time.time()\n","    t_elapsed_epoch = t_end_epoch - t_start_epoch\n","    print('\\nAverage loss = {:05.3f}\\n'.format(avg_loss))\n","    total_loss.append(avg_loss)\n","    logger.add('(Fold {}) Epoch {}/{}'.format(fold,epoch,epochs),'Average loss = {:05.3f} ({:05.3f} sec)'.format(avg_loss,t_elapsed_epoch))\n","\n","    if config[\"scheduler\"]:\n","      scheduler.step()\n","\n","  t_end_train = time.time()\n","  t_elapsed_train = t_end_train - t_start_train\n","  print('Total Execution Time Fold {}: {:05.2f} minutes'.format(fold,t_elapsed_train/60))\n","  logger.add('Total Execution Time Fold {}'.format(fold),'{:05.2f} minutes'.format(t_elapsed_train/60))\n","\n","  # VALIDATION\n","  print('\\n{:=^60}\\n'.format(' VALIDATION '))\n","  accuracy, targets_true, targets_pred = evaluate_AudioNet(model=model, device=device, dataloader=val_dataloader)\n","  print('\\nAccuracy = {:05.3f}\\n'.format(accuracy))\n","\n","  logger.add('VALIDATION Fold {}'.format(fold),'\\n{:=^60}'.format(''))\n","  logger.add('Validation Accuracy Fold {}'.format(fold),'{:05.3f} %'.format(accuracy))\n","\n","  # METRICS\n","  create_metrics(config,targets_true, targets_pred, fold, save_metrics=config[\"save_metrics\"])\n","  plot_train_loss(epochs,total_loss,config,fold,save_metrics=config[\"save_metrics\"])\n","\n","  # Saving the final model\n","  if save_model == True:\n","    path = config[\"saved_models_path\"]+config[\"model_folder_name\"]+'/'\n","    if config[\"pretrained\"] == True:\n","      torch.save(model.state_dict(), \"{}AudioNetTL_Fold{}.bin\".format(path,fold))\n","    else:\n","      torch.save(model.state_dict(), \"{}AudioNetRW_Fold{}.bin\".format(path,fold))\n","\n","  return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b4rorK0hiljd"},"source":["# Create AudioNetTL (Pretrained)\n","\n","Train and evaluate the pretrained version of AudioNet"]},{"cell_type":"code","metadata":{"id":"sykT15Syje9n"},"source":["num_epochs = [2,4,6,8]\n","\n","for epochs in num_epochs:\n","\n","  # Create Log File\n","  LOG = Logger()\n","\n","  # Connect to google drive\n","  drive.mount('/content/gdrive')\n","\n","  device, GPU_name = get_GPU_device()\n","\n","  # Define parameters for configuration\n","  config = {\"pretrained\": True,\n","            \"num_classes\": 4,\n","            \"num_folds\": 5,\n","            \"epochs\": epochs,\n","            \"device\": device,\n","            \"test_split\": 0.2,\n","            \"learning_rate\": 2e-5,\n","            \"scheduler\": True,\n","            \"batch_size\": 4,\n","            \"random_state\": 2021,\n","            \"data_path\": '/path/to/combined_dataset.pkl', # Change in order to run the script \n","            \"saved_models_path\":'/path/to/saved/models', # Change in order to run the script\n","            \"save_model\": False,\n","            \"save_metrics\": True}\n","\n","  config[\"case\"] = \"TL{}_3x224x224_MAXLEN_160\".format(config[\"epochs\"])\n","\n","  data = load_pkl_data(config[\"data_path\"])\n","\n","  # Prepare data for cross-validation\n","  data = create_kfold_CV(data=data,num_folds=config[\"num_folds\"],random_state=config[\"random_state\"],shuffle=True)\n","\n","  LOG.add(\"Run Name\",\"FULL RUN.\")\n","  LOG.add(\"Date/Time\",datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S (UTC)\"))\n","  LOG.add(\"config\",config)\n","  LOG.add(\"case\",config[\"case\"])\n","  LOG.add(\"GPU\",GPU_name)\n","\n","  LOG.add(\"Libraries\",'')\n","  LOG.add(\"PyTorch version\",torch.__version__)\n","  LOG.add(\"TensorFlow version\",tf.__version__)\n","  LOG.add(\"Transformers version\",transformers.__version__)\n","  LOG.add(\"Sklearn version\",sklearn.__version__)\n","  LOG.add(\"Numpy version\",np.__version__)\n","  LOG.add(\"Matplotlib version\",matplotlib.__version__)\n","\n","  accuracy_allfolds = []\n","\n","  # Create folder for current model\n","  config[\"model_folder_name\"] = create_model_folder_name(config[\"case\"],config[\"saved_models_path\"])\n","\n","\n","  t_start = time.time()\n","  with tqdm(total=config[\"num_folds\"]) as t:\n","    for fold in range(1,config[\"num_folds\"]+1):\n","      # Create model\n","      model = AudioNet(num_classes=config[\"num_classes\"],pretrained=config[\"pretrained\"]).to(device)\n","      if fold == 1:\n","        LOG.add(\"Model\",model)\n","\n","      train_data,test_data = get_train_val_folds(data,fold)\n","\n","      accuracy_single_fold = train_eval_AudioNet(model,device,train_data,test_data,config,logger=LOG,fold=fold,save_model=config[\"save_model\"])\n","\n","      accuracy_allfolds.append(accuracy_single_fold)\n","\n","      # Empty CUDA cache after each fold to avoid GPU out-of-memory issues\n","      torch.cuda.empty_cache()\n","      print('\\nCUDA cache emptied!')\n","\n","      t.update()\n","\n","  print('\\nMean Accuracy = {:05.3f}'.format(np.mean(accuracy_allfolds)))\n","  LOG.add('OVERALL ACCURACY','\\n{:=^60}'.format(''))\n","  LOG.add('Accuracy All Folds',accuracy_allfolds)\n","  LOG.add('Mean Accuracy','{:05.3f} %'.format(np.mean(accuracy_allfolds)))\n","\n","  t_end = time.time()\n","  t_elapsed = t_end - t_start\n","  LOG.add('Total Execution Time','{:05.2f} minutes'.format(t_elapsed/60))\n","\n","  # Save Log File to Google Drive\n","  if config[\"save_metrics\"]:\n","    path = config[\"saved_models_path\"]+config[\"model_folder_name\"]+'/'\n","    LOG.save_log(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFMNYBPUio7K"},"source":["# Create AudioNetRW (Random Weights)\n","\n","Train and evalute the AudioNet without Transfer Learning (weights are randomly initialised)."]},{"cell_type":"code","metadata":{"id":"cGpGyLQ-Rzv0"},"source":["num_epochs = [2,4,6,8]\n","\n","for epochs in num_epochs:\n","\n","  # Create Logger Object for Log File\n","  LOG = Logger()\n","\n","  # Connect to google drive\n","  drive.mount('/content/gdrive')\n","\n","  device, GPU_name = get_GPU_device()\n","\n","  # Define parameters\n","  config = {\"pretrained\": False,\n","            \"num_classes\": 4,\n","            \"num_folds\": 5,\n","            \"epochs\": epochs,\n","            \"device\": device,\n","            \"test_split\": 0.2,\n","            \"learning_rate\": 2e-5,\n","            \"scheduler\": True,\n","            \"batch_size\": 4,\n","            \"random_state\": 2021,\n","            \"data_path\": '/path/to/combined_dataset.pkl', # Change in order to run the script \n","            \"saved_models_path\":'/path/to/saved/models', # Change in order to run the script\n","            \"save_model\": False,\n","            \"save_metrics\": True}\n","\n","  config[\"case\"] = \"RW{}_3x224x224_MAXLEN_160\".format(config[\"epochs\"])\n","\n","  data = load_pkl_data(config[\"data_path\"])\n","\n","  # Create Cross-Validation indizes\n","  data = create_kfold_CV(data=data,num_folds=config[\"num_folds\"],random_state=config[\"random_state\"],shuffle=True)\n","\n","  LOG.add(\"Run Name\",\"FULL RUN.\")\n","  LOG.add(\"Date/Time\",datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S (UTC)\"))\n","  LOG.add(\"config\",config)\n","  LOG.add(\"case\",config[\"case\"])\n","  LOG.add(\"GPU\",GPU_name)\n","\n","  LOG.add(\"Libraries\",'')\n","  LOG.add(\"PyTorch version\",torch.__version__)\n","  LOG.add(\"TensorFlow version\",tf.__version__)\n","  LOG.add(\"Transformers version\",transformers.__version__)\n","  LOG.add(\"Sklearn version\",sklearn.__version__)\n","  LOG.add(\"Numpy version\",np.__version__)\n","  LOG.add(\"Matplotlib version\",matplotlib.__version__)\n","\n","  accuracy_allfolds = []\n","\n","  # Create folder for current model\n","  config[\"model_folder_name\"] = create_model_folder_name(config[\"case\"],config[\"saved_models_path\"])\n","\n","\n","  t_start = time.time()\n","  with tqdm(total=config[\"num_folds\"]) as t:\n","    for fold in range(1,config[\"num_folds\"]+1):\n","      # Create model\n","      model = AudioNet(num_classes=config[\"num_classes\"],pretrained=config[\"pretrained\"]).to(device)\n","      if fold == 1:\n","        LOG.add(\"Model\",model)\n","\n","      train_data,test_data = get_train_val_folds(data,fold)\n","\n","      accuracy_single_fold = train_eval_AudioNet(model,device,train_data,test_data,config,logger=LOG,fold=fold,save_model=config[\"save_model\"])\n","\n","      accuracy_allfolds.append(accuracy_single_fold)\n","\n","      # Empty CUDA cache after each fold to avoid GPU out-of-memory issues\n","      torch.cuda.empty_cache()\n","      print('\\nCUDA cache emptied!')\n","\n","      t.update()\n","\n","  print('\\nMean Accuracy = {:05.3f}'.format(np.mean(accuracy_allfolds)))\n","  LOG.add('OVERALL ACCURACY','\\n{:=^60}'.format(''))\n","  LOG.add('Accuracy All Folds',accuracy_allfolds)\n","  LOG.add('Mean Accuracy','{:05.3f} %'.format(np.mean(accuracy_allfolds)))\n","\n","  t_end = time.time()\n","  t_elapsed = t_end - t_start\n","  LOG.add('Total Execution Time','{:05.2f} minutes'.format(t_elapsed/60))\n","\n","  # Save Log File to Google Drive\n","  if config[\"save_metrics\"]:\n","    path = config[\"saved_models_path\"]+config[\"model_folder_name\"]+'/'\n","    LOG.save_log(path)"],"execution_count":null,"outputs":[]}]}